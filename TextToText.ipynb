{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9970130,"sourceType":"datasetVersion","datasetId":6133697},{"sourceId":10074019,"sourceType":"datasetVersion","datasetId":6209579}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport unicodedata\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention, Concatenate\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:47:24.274147Z","iopub.execute_input":"2024-12-03T14:47:24.274524Z","iopub.status.idle":"2024-12-03T14:47:24.280217Z","shell.execute_reply.started":"2024-12-03T14:47:24.274490Z","shell.execute_reply":"2024-12-03T14:47:24.279372Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(tf.config.list_physical_devices('GPU'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:47:24.281596Z","iopub.execute_input":"2024-12-03T14:47:24.281851Z","iopub.status.idle":"2024-12-03T14:47:24.292337Z","shell.execute_reply.started":"2024-12-03T14:47:24.281827Z","shell.execute_reply":"2024-12-03T14:47:24.291500Z"}},"outputs":[{"name":"stdout","text":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n\ndef create_model(input_vocab_size, target_vocab_size, embedding_dim, hidden_units, dropout_rate, learning_rate):\n    # Encoder input\n    encoder_input = Input(shape=(None,), dtype=tf.int32, name='encoder_input')\n    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_input)\n    encoder_lstm = LSTM(hidden_units, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n\n    # Decoder input\n    decoder_input = Input(shape=(None,), dtype=tf.int32, name='decoder_input')\n    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_input)\n    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)\n    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n    \n    # Output layer\n    output = Dense(target_vocab_size, activation='softmax')(decoder_outputs)\n    \n    # Create and compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model = Model([encoder_input, decoder_input], output)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Example usage\ninput_vocab_size = 30000  # example value\ntarget_vocab_size = 30000 # example value\nembedding_dim = 256\nhidden_units = 512\ndropout_rate = 0.3\nlearning_rate = 0.0005\n\nmodel = create_model(input_vocab_size, target_vocab_size, embedding_dim, hidden_units, dropout_rate, learning_rate)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:47:24.293642Z","iopub.execute_input":"2024-12-03T14:47:24.293928Z","iopub.status.idle":"2024-12-03T14:47:24.627983Z","shell.execute_reply.started":"2024-12-03T14:47:24.293903Z","shell.execute_reply":"2024-12-03T14:47:24.627217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m7,680,000\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m7,680,000\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │                   │\n│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m15,390,000\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│                     │ \u001b[38;5;34m30000\u001b[0m)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,680,000</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,680,000</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │                   │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390,000</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)            │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,899,824\u001b[0m (129.32 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,899,824</span> (129.32 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,899,824\u001b[0m (129.32 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,899,824</span> (129.32 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":" # Indonesian sentences file\n\ndef load_and_preprocess(file_en, file_id):\n    # Load files with UTF-8 decoding\n    with open(file_en, 'r', encoding='utf-8') as f_en, open(file_id, 'r', encoding='utf-8') as f_id:\n        english_lines = f_en.readlines()\n        indonesian_lines = f_id.readlines()   \n    english_lines=english_lines[:500000]\n    indonesian_lines=indonesian_lines[:500000]\n    # Ensure line counts match\n    print(len(english_lines))\n    print(len(indonesian_lines))\n    \n    assert len(english_lines) == len(indonesian_lines), \"Mismatched line counts in dataset!\"\n\n\n    # Preprocess lines\n    english_lines = ['<start> ' + line.strip().lower() + ' <end>' for line in english_lines]\n    indonesian_lines = ['<start> ' + line.strip().lower() + ' <end>' for line in indonesian_lines]\n    \n    return english_lines, indonesian_lines\n\n# Tokenize and pad sequences\ndef tokenize_and_pad(lines, num_words, max_length):\n    tokenizer = Tokenizer(num_words=num_words, filters='')\n    tokenizer.fit_on_texts(lines)\n    sequences = tokenizer.texts_to_sequences(lines)\n    padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n    return tokenizer, padded\n\n# Load dataset\nfile_en = \"/kaggle/input/opensub-en-id/OpenSubtitles.en-id.en\"  # English sentences file\nfile_id = \"/kaggle/input/opensub-en-id/OpenSubtitles.en-id.id\" \n\nenglish_lines, indonesian_lines = load_and_preprocess(file_en, file_id)\n\n# Tokenize and pad\nnum_words = 30000  # Maximum vocabulary size\nmax_length = 50    # Maximum sentence length\n\ninput_tokenizer, input_padded = tokenize_and_pad(english_lines, num_words, max_length)\noutput_tokenizer, output_padded = tokenize_and_pad(indonesian_lines, num_words, max_length)\n\n# Target data needs to be shifted for teacher forcing\noutput_input = output_padded[:, :-1]  # Remove last token for decoder input\noutput_target = output_padded[:, 1:]  # Remove first token for target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:47:24.628877Z","iopub.execute_input":"2024-12-03T14:47:24.629106Z","iopub.status.idle":"2024-12-03T14:47:44.384818Z","shell.execute_reply.started":"2024-12-03T14:47:24.629083Z","shell.execute_reply":"2024-12-03T14:47:44.383797Z"}},"outputs":[{"name":"stdout","text":"500000\n500000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"batch_size = 64\nepochs = 5\n\nhistory = model.fit(\n    [input_padded, output_input],\n    output_target[..., np.newaxis],  # Add extra dimension for sparse categorical crossentropy\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2\n)\nmodel.save(\"MT.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T14:47:44.386867Z","iopub.execute_input":"2024-12-03T14:47:44.387580Z","iopub.status.idle":"2024-12-03T17:38:03.311556Z","shell.execute_reply.started":"2024-12-03T14:47:44.387534Z","shell.execute_reply":"2024-12-03T17:38:03.310564Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2036s\u001b[0m 325ms/step - accuracy: 0.9020 - loss: 0.8712 - val_accuracy: 0.9249 - val_loss: 0.5482\nEpoch 2/5\n\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2044s\u001b[0m 327ms/step - accuracy: 0.9221 - loss: 0.5321 - val_accuracy: 0.9322 - val_loss: 0.4761\nEpoch 3/5\n\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2044s\u001b[0m 327ms/step - accuracy: 0.9292 - loss: 0.4488 - val_accuracy: 0.9353 - val_loss: 0.4452\nEpoch 4/5\n\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2046s\u001b[0m 327ms/step - accuracy: 0.9332 - loss: 0.3989 - val_accuracy: 0.9370 - val_loss: 0.4292\nEpoch 5/5\n\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2047s\u001b[0m 328ms/step - accuracy: 0.9362 - loss: 0.3626 - val_accuracy: 0.9381 - val_loss: 0.4202\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def beam_search_decode(input_sequence, model, input_tokenizer, output_tokenizer, max_length, beam_width=3):\n    # Initialize variables\n    start_token = output_tokenizer.word_index['<start>']\n    end_token = output_tokenizer.word_index['<end>']\n    sequences = [[list(), 0.0]]  # Each sequence: (tokens, score)\n\n    # Preprocess input\n    input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n\n    for _ in range(max_length):\n        all_candidates = []\n        for seq, score in sequences:\n            if seq and seq[-1] == end_token:\n                all_candidates.append((seq, score))\n                continue\n            target_sequence = pad_sequences([seq], maxlen=max_length, padding='post')\n            predictions = model.predict([input_sequence, target_sequence], verbose=0)\n            for i in np.argsort(predictions[0, len(seq) - 1])[-beam_width:]:\n                candidate = seq + [i]\n                candidate_score = score - np.log(predictions[0, len(seq) - 1, i])\n                all_candidates.append((candidate, candidate_score))\n        sequences = sorted(all_candidates, key=lambda tup: tup[1])[:beam_width]\n\n    final_sequence = sequences[0][0]\n    translated_sentence = ' '.join([output_tokenizer.index_word.get(token, '') for token in final_sequence if token != start_token and token != end_token])\n    return translated_sentence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:38:03.313007Z","iopub.execute_input":"2024-12-03T17:38:03.313393Z","iopub.status.idle":"2024-12-03T17:38:03.322827Z","shell.execute_reply.started":"2024-12-03T17:38:03.313352Z","shell.execute_reply":"2024-12-03T17:38:03.321714Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def translate_sentence(input_sentence, model, input_tokenizer, output_tokenizer, max_length):\n    # Preprocess input sentence\n    input_sentence = '<start> ' + input_sentence.strip().lower() + ' <end>'\n    input_sequence = input_tokenizer.texts_to_sequences([input_sentence])\n    input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n    \n    # Initialize the decoder input with <start> token\n    start_token = output_tokenizer.word_index['<start>']\n    end_token = output_tokenizer.word_index['<end>']\n    target_sequence = np.zeros((1, max_length))\n    target_sequence[0, 0] = start_token\n    \n    translated_sentence = []\n    for i in range(1, max_length):\n        # Predict next token\n        predictions = model.predict([input_sequence, target_sequence], verbose=0)\n        predicted_id = np.argmax(predictions[0, i - 1])\n        \n        # Stop if <end> token is predicted\n        if predicted_id == end_token:\n            break\n        \n        # Append the predicted word\n        translated_sentence.append(output_tokenizer.index_word.get(predicted_id, ''))\n        target_sequence[0, i] = predicted_id\n    \n    return ' '.join(translated_sentence)\n\n# Example usage\nmanual_input = input(\"Masukan kalimat: \")\ntranslated_output = translate_sentence(manual_input, model, input_tokenizer, output_tokenizer, max_length)\ndebug = beam_search_decode(input_padded[0:1], model, input_tokenizer, output_tokenizer, max_length)\nprint(\"Input:\", manual_input)\nprint(\"Translated Output:\", translated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:38:03.324097Z","iopub.execute_input":"2024-12-03T17:38:03.324455Z","iopub.status.idle":"2024-12-03T17:55:19.316908Z","shell.execute_reply.started":"2024-12-03T17:38:03.324418Z","shell.execute_reply":"2024-12-03T17:55:19.316001Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Masukan kalimat:  this is a test\n"},{"name":"stdout","text":"Input: this is a test\nTranslated Output: ini adalah sebuah\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def test_translate_sentence(model, input_tokenizer, output_tokenizer, max_length):\n    test_cases = [\n        # Simple test cases (common phrases)\n        (\"hello\", \"halo\"),  # Should translate to the expected Indonesian translation\n        (\"how are you\", \"apa kabar\"),\n        (\"this is good\", \"ini bagus\"),\n        \n        # More complex sentences\n        (\"nothing to do here\", \"tidak ada yang bisa dilakukan disini\"),\n        (\"the weather is nice today\", \"cuaca bagus hari ini\"),\n        \n        # Edge cases (sentences that may be harder for the model)\n        (\"I am happy\", \"saya senang\"),\n        (\"good morning\", \"selamat pagi\"),\n        \n        # Test for empty input (should ideally return an empty string or a specific error message)\n        (\"\", \"\"),\n    ]\n    \n    # Run test cases\n    for input_sentence, expected_output in test_cases:\n        print(f\"Testing input: {input_sentence}\")\n        \n        translated_output = translate_sentence(input_sentence, model, input_tokenizer, output_tokenizer, max_length)\n        print(f\"Expected output: {expected_output}\")\n        print(f\"Model output: {translated_output}\")\n# Run the test\ntest_translate_sentence(model, input_tokenizer, output_tokenizer, max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:55:19.318008Z","iopub.execute_input":"2024-12-03T17:55:19.318291Z","iopub.status.idle":"2024-12-03T17:55:21.907681Z","shell.execute_reply.started":"2024-12-03T17:55:19.318265Z","shell.execute_reply":"2024-12-03T17:55:21.906737Z"}},"outputs":[{"name":"stdout","text":"Testing input: hello\nExpected output: halo\nModel output: hello\nTesting input: how are you\nExpected output: apa kabar\nModel output: bagaimana anda\nTesting input: this is good\nExpected output: ini bagus\nModel output: ini adalah baik.\nTesting input: nothing to do here\nExpected output: tidak ada yang bisa dilakukan disini\nModel output: tidak ada yang perlu di sini.\nTesting input: the weather is nice today\nExpected output: cuaca bagus hari ini\nModel output: hari ini sangat baik hari ini.\nTesting input: I am happy\nExpected output: saya senang\nModel output: aku senang\nTesting input: good morning\nExpected output: selamat pagi\nModel output: selamat pagi.\nTesting input: \nExpected output: \nModel output: \n","output_type":"stream"}],"execution_count":16}]}